---
title: "Statistics Project"
author: "davidabraham"
date: "2/15/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Importing in R

```{r}
Student.mat<-read.table('~/Desktop/Shipwreck/student-mat.csv',sep = ",")
student.mat<-read.csv('~/Desktop/Shipwreck/student-mat.csv',sep = ",")
head(Student.mat)
```
\newpage
# Q1. Analysis of the effect on Final Grade(G3) as a result of G1 and G2

Goal:-Predict students ability to pass and their grades based on certain variables and find which variable(s) is the best predictor.Dataset used is student-mat.csv

Student dataset has G3 variable which is used for classifying Pass/Fail and for actegorising student's grades into Fail,Sufficient,Satisfactory,Good and Excellent.These classifications will be predicted based on some independent variables.

Predictors are:-
  1. ParentStatus(living together or not)
  2. Mother??? s education(factors:- none, upto 4th grade, upto 9th grade, secondary education and higher education)
  3. Travel time to school
  4. Romantic status of the student
  5. G1 - score from test1
  6. G2 - score from test2
  
Different methods used for prediction  :- 
1. Linear regression 
2. Decision Tree
3. Naive Bayes Method


1. Linear regression is used on variables G1 and G2 individually to predict G3
2. Decision tree is used on variables G1 and G2 together to predict pass-ability and Grades
3. Naive Bayes method is used on categorical variables - ParentStatus, MotherEducation, TravelTime and Romantic Status to predict pass-ability and grades.

## 1. Load students data

```{r}
students<-read.table('~/Desktop/Shipwreck/student-mat.csv',header = TRUE,sep = ",")
```
## 2. Extract necessary columns for analysis

```{r}
students<- students[,c(6,7,13,23,31,32,33)]
```
## 3. Calculate pass or fail variable and store it in variable Pass

```{r}
Pass <- ifelse(students$G3>9,'PASS','FAIL')
students <- data.frame(students,Pass)
```
## 4. Calculate Grade variable

```{r}
Grade <- ifelse(students$G3<=9,'FAIL','PASS')
Grade <- ifelse(students$G3>=10 & students$G3<=11,'Sufficient',Grade)
Grade <- ifelse(students$G3>=12 & students$G3<=13,'Satisfactory',Grade)
Grade <- ifelse(students$G3>=14 & students$G3<=15,'Good',Grade)
Grade <- ifelse(students$G3>=16 & students$G3<=20,'Excellent',Grade)
students <- data.frame(students,Grade)
```
## 5. Exploration of data 

### Dimensions
```{r}
dim(students)
```
### Number of rows in data
```{r}
nrow(students)
```
### Number of columns
```{r}
ncol(students)
```
### Structure 
```{r}
str(students)
```
### Variable or column names 
```{r}
names(students)
```
### Attributes
```{r}
attributes(students)
```
### Top 10 rows
```{r}
head(students,n=10)
```
### Variable distribution before factorisation
```{r}
summary(students)
```

## Factorize continuous predictor variables

### Variable distribution after necessary factorisation

```{r}
students$Medu <- factor(students$Medu)
students$traveltime <- factor(students$traveltime)
summary(students)
```

## Pie chart for pass

```{r ,fig.width=5, fig.height=5, message=FALSE, results = 'asis', echo=FALSE}
pie(with(students, table(Pass)))
```

## Pie chart for grade


```{r ,fig.width=5, fig.height=5, message=FALSE, results = 'asis', echo=FALSE}
pie(with(students, table(Grade)))
```
\newpage

##  Bar graph for pass

```{r ,fig.width=5, fig.height=5, message=FALSE, results = 'asis', echo=FALSE}
library(ggplot2)
ggplot(students, aes(x = Pass)) + geom_bar(fill="#FF9999")
```

## Bar graph for grade

```{r ,fig.width=5, fig.height=5, message=FALSE, results = 'asis', echo=FALSE}
reordervect <- rep(0,nrow(students))
reordervect[with(students, Grade == "Fail")] = 1
reordervect[with(students, Grade == "Sufficient")] = 2
reordervect[with(students, Grade == "Satisfactory")] = 3
reordervect[with(students, Grade == "Good")] = 4
reordervect[with(students, Grade == "Excellent")] = 5
students$Grade = with(students, reorder(Grade, reordervect))
rm(reordervect)
ggplot(students, aes(x = Grade)) + geom_bar(fill="#FF9999") 
```

### Statistical data of G3

```{r}
summary(with(students,G3))
sprintf('variance is %f',var(with(students,G3)))
sprintf('standard deviation is %f',sd(with(students,G3)))
```

### Histogram for G3

```{r,echo=FALSE}
ggplot(students, aes(x=G3)) + geom_histogram(fill="#FF9999")
```

## Predicting G3 using G1

### Correlation between G1 and G3

```{r}
r <- cor(with(students,G1), with(students,G3))
sprintf("G3 shows a positive correllation with G1")
```

### Scatterplot of G1, G3

```{r,echo=FALSE}
ggplot(students, aes(x=G1,y=G3)) + geom_point() + geom_smooth(method="lm", se=FALSE)
```

### Fit linear regression using G1 as predictor to predict G3

```{r}
fit <- with(students,lm(G3 ~ G1))
fit
attributes(fit)
summary(fit)
```

## Plotting line of best fit

```{r,echo=FALSE}
plot(fit)
sprintf("Residual graph is random in nature suggesting linear regression is not a bad choice for this data")
```

## Predicting G3 using G2

### Correlation between G2 and G3

```{r}
r <- cor(with(students,G2), with(students,G3))
sprintf('Correlation between G2 and G3 is %f and the coefficient of determination is %f',r, r^2)
```

### Scatterplot of G2, G3

```{r,echo=FALSE}
ggplot(students, aes(x=G2,y=G3)) + geom_point() + geom_smooth(method="lm", se=FALSE)
```

### Fit linear regression using G1 as predictor to predict G3

```{r}
fit <- with(students,lm(G3 ~ G2))
fit
attributes(fit)
summary(fit)
```

### Plotting line of best fit

```{r,echo=FALSE}
plot(fit)
sprintf("Residual graph is random in nature suggesting linear regression is not a bad choice for this data")
```

## To predict  Pass and Fail using G1 + G2

### Using Decision Tree

```{r}
library(partykit)
formula <- Pass ~ G1 + G2
tree <- ctree(formula, data=students)
print(tree)
plot(tree,type = "simple")
```
```{r}
sprintf('Errors-on-predictions Matrix')
table(predict(tree, newdata=students), students$Pass,dnn=c('Predicted','Actual'))
df.confmatrix <- data.frame(table(predict(tree, newdata=students), students$Pass,dnn=c('Predicted','Actual')))
library(tidyr)
data_long <- gather(df.confmatrix, Type, Status, Predicted:Actual)
library(dplyr)
data_long <- data_long %>% group_by(Status,Type) %>% summarise(Frequency=sum(Freq))
ggplot(data_long, aes(x=Status,y=Frequency,fill=Type)) + geom_bar(stat='identity', position='dodge')
```

### Using Naive-Bayes Prediction

```{r}
library(e1071)
classifier<-naiveBayes(students[,1:4], students[,8])
table(predict(classifier, students[,1:4]), students[,8], dnn = c('Predicted','Actual'))
df.confmatrix <- data.frame(table(predict(classifier, students[,1:4]), students[,8], dnn = c('Predicted','Actual')))
data_long <- gather(df.confmatrix, Type, Status, Predicted:Actual)
data_long <- data_long %>% group_by(Status,Type) %>% summarise(Frequency=sum(Freq))
ggplot(data_long, aes(x=Status,y=Frequency,fill=Type)) + geom_bar(stat='identity', position='dodge')
```

## To predict Grades using G1+G3

### Using Decision Tree

```{r}
formula <- Grade ~ G1 + G2
tree <- ctree(formula, data=students)
print(tree)
plot(tree,type = "simple")
```
```{r}
sprintf('Errors-on-predictions Matrix')
table(predict(tree, newdata=students), students$Grade,dnn=c('Predicted','Actual'))
df.confmatrix <- data.frame(table(predict(tree, newdata=students), students$Grade,dnn=c('Predicted','Actual')))
data_long <- gather(df.confmatrix, Type, Status, Predicted:Actual)
data_long <- data_long %>% group_by(Status,Type) %>% summarise(Frequency=sum(Freq))
ggplot(data_long, aes(x=Status,y=Frequency,fill=Type)) + geom_bar(stat='identity', position='dodge')
```

### Using Naive-Bayes Prediction

```{r}
classifier<-naiveBayes(students[,1:4], students[,9])
sprintf('Errors-on-predictions Matrix')
table(predict(classifier, students[,1:4]), students[,9], dnn = c('Predicted','Actual'))
df.confmatrix <- data.frame(table(predict(classifier, students[,1:4]), students[,9], dnn = c('Predicted','Actual')))
data_long <- gather(df.confmatrix, Type, Status, Predicted:Actual)
data_long <- data_long %>% group_by(Status,Type) %>% summarise(Frequency=sum(Freq))
ggplot(data_long, aes(x=Status,y=Frequency,fill=Type)) + geom_bar(stat='identity', position='dodge') 
```
\newpage
#  Q2. What is the impact of age and the sex on performance(G3) ?

First of all, I checked whether there is a difference in performance between boys and girls.

```{r}
gender.dif <- t.test(student.mat$G3~student.mat$sex,var.equal = TRUE)
library(apa)
apa(gender.dif)
```

The mean values between the genders is not equal.Now, I go a step further and take also the age into consideration.

```{r}
summary(with(student.mat, aov(G3 ~ sex + age + sex*age)))
```
```{r}
with(student.mat, interaction.plot(sex, age, G3))
with(student.mat, interaction.plot(age, sex, G3))
```
```{r}
table(student.mat$age)
student.mat.2 <- subset(student.mat, age < 20)
table(student.mat.2$age)
```
```{r}
summary(with(student.mat.2, aov(G3 ~ sex + age + sex*age)))
```

While eliminating the outliers, the probability that the treatment means differ became less likely for every factor. Now, I redo the interaction plots, too.

```{r}
with(student.mat.2, interaction.plot(sex, age, G3))
with(student.mat.2, interaction.plot(age, sex, G3))
```

Especially the last plot looks better. But as it seems, the interaction is more difficult to understand. I was wondering why the performance of boys gradually (linearly) decreases when boys grow older and why the performance of girls stays more constant with reference to the age. Looking at the mean values this thought is reinforced.

```{r}
library(tidyverse)
student.mat.2 %>%
  group_by(age, sex) %>%
  summarise(
    a.mean = mean(G3)
  )
```

Consequently, I checked the correlation between age and the performance for two subsets holding boys and girls separately.

```{r}
cor.test1 <- with(subset(student.mat.2, sex == "M"),
     cor.test(G3, age))
cor.test1
apa(cor.test1)
cor.test2 <- with(subset(student.mat.2, sex == "F"),
     (cor.test(G3, age)))
cor.test2
apa(cor.test2)
```

As expected, there is a correlation between age and performance for boys and none for girls. Therefore, I will focus on the boys. I calculate a linear regression analysis between age and performance in order to get further information.

```{r}
with(subset(student.mat.2, sex == "M"),
     summary(lm(G3~ age)))
```

The probability that the group means are equal is, of course, the same as in the correlation analysis. However, with the linear regression we can predict values and show a tendency with a regression line. Last but not least, I show the results in a scatter plot:

```{r}
plot(1,
     xlim = c(15, 19),
     ylim = c(0, 20),
     type = "n",
     main = "Relationship between age and performance",
     xlab = "Age",
     ylab = "Performance in G3"
     )

#Now, I fill in the points.
with(subset(student.mat.2, sex == "M"), 
     points(age, 
            G3,
            pch = 25, 
            col = alpha("blue", 0.2)
            ))

#Finally, I draw the regression line.
with(subset(student.mat.2, sex == "M"),
     abline(lm(G3 ~ age), col = "blue"))
```
\newpage

# Q3. What is the relationship between failures and performance with reference to the age?

While eliminating persons older than 20, I recognized that these persons have bad grades. So at first, I checked the correlation between failures and age.

```{r}
cor.test3 <- with(student.mat, cor.test(age, failures))
apa(cor.test3)
cor.test2 <- with(subset(student.mat.2, sex == "F"),
     (cor.test(G3, age)))
cor.test2
```

The results reveal a strong connection between failures and age. This maybe explains why there are people of 22 in a school class. Furthermore, I explored the relationship between age, failures and the performance

```{r}
with(student.mat, summary(aov(G3 ~ age + failures + age*failures)))
```

The results show that all factors are significant. The older a person was and the more failures a person experienced, the more will the performance decrease. Finally, I plot the results:

```{r}
plot(1,
     xlim = c(15, 22),
     ylim = c(0, 20),
     type = "n",
     main = "Relationship between age and performance",
     xlab = "Age",
     ylab = "Performance in G3"
     )

#People with no failures.
with(subset(student.mat, failures == 0), 
     points(age, 
            G3,
            pch = 21, 
            col = alpha("blue", 0.1),
            bg =alpha("blue", 0.1)
            ))

#People with more than one failure. 
with(subset(student.mat, failures > 0), 
     points(age, 
            G3,
            pch = 21, 
            col = alpha("red", 0.1),
            bg =alpha("red", 0.1)
            ))

with(student.mat, abline(lm(G3 ~ age + failures + age*failures)))
```
\newpage

# Q4. Relationship between goout and performance

```{r}
lm1 <- with(student.mat, summary(lm(G3 ~ goout )))
lm1
```

Going out is significantly related to the performance in the math course. I want to visualize this with a scatter plot:

```{r}
plot(1,
     xlim = c(1, 5),
     ylim = c(0, 20),
     type = "n",
     main = "Relationship between goout and performance",
     xlab = "Goout",
     ylab = "Performance in G3"
     )


with(student.mat, 
     points(goout, 
            G3,
            pch = 21, 
            col = alpha("blue", 0.1),
            bg =alpha("blue", 0.1)
            ))

with(student.mat, abline(lm(G3 ~ goout)))
```

After checking the plot I realized that the mean of the performance is low when the child is rarely going out. This is why I assumed another coherence. At first, I checked for the means:

```{r}
aggregate(
  formula = G3 ~ goout,
  data= student.mat, 
  FUN = mean)
```

The means reveal what I assumed. The first mean is lower than the second or third one. Finally, I expected the relationship between performance and going out to be quadratic. I checked this with a regression analysis.
\newpage

##  Conclusion

Linear regression showed strong relationship between G3 and G2. G1 also showed positive relationship but not as strong as G2.
Decision tree prediction on the same dataset showed very less errors on predictions making G1 and G2 suitable for predicting Grades and Pass-ability of students
Naive Bayes method showed large errors on predictions. So, either the those four variables are not good predictors or Naive Bayes method is not a good predicting model for this dataset.
Based on all the analysis, G2 is the strongest predictor for G3, which in turn, for pass-ability and grades.

In comparison to that the result that going out is negatively correlated with your performance in a Math Class is totally intuitive. Additionally, the results revealed that older children which failed once or several times have lower performance rates.

While boys show lower performances when they grow older, girls remain relatively constant.
