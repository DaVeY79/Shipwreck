---
title: "Shipwreck student-mat"
author: "davidabraham"
date: "2/14/2017"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# 1. Investigation into G3

Goal:-Predict students ability to pass and their grades based on certain variables and find which variable(s) is the best predictor.Dataset used is student-mat.csv

Student dataset has G3 variable which is used for classifying Pass/Fail and for actegorising student's grades into Fail,Sufficient,Satisfactory,Good and Excellent.These classifications will be predicted based on some independent variables.

Predictors are:-
  1. ParentStatus(living together or not)
  2. Mother??? s education(factors:- none, upto 4th grade, upto 9th grade, secondary education and higher education)
  3. Travel time to school
  4. Romantic status of the student
  5. G1 - score from test1
  6. G2 - score from test2
  
Different methods used for prediction  :- 
1. Linear regression 
2. Decision Tree
3. Naive Bayes Method


1. Linear regression is used on variables G1 and G2 individually to predict G3
2. Decision tree is used on variables G1 and G2 together to predict pass-ability and Grades
3. Naive Bayes method is used on categorical variables - ParentStatus, MotherEducation, TravelTime and Romantic Status to predict pass-ability and grades.

## 1. Load students data
```{r}
students<-read.table('~/Desktop/Shipwreck/student-mat.csv',header = TRUE,sep = ",")
```
## 2. Extract necessary columns for analysis
```{r}
students<- students[,c(6,7,13,23,31,32,33)]
```
## 3. Calculate pass or fail variable and store it in variable Pass
```{r}
Pass <- ifelse(students$G3>9,'PASS','FAIL')
students <- data.frame(students,Pass)
```
## 4. Calculate Grade variable
```{r}
Grade <- ifelse(students$G3<=9,'FAIL','PASS')
Grade <- ifelse(students$G3>=10 & students$G3<=11,'Sufficient',Grade)
Grade <- ifelse(students$G3>=12 & students$G3<=13,'Satisfactory',Grade)
Grade <- ifelse(students$G3>=14 & students$G3<=15,'Good',Grade)
Grade <- ifelse(students$G3>=16 & students$G3<=20,'Excellent',Grade)
students <- data.frame(students,Grade)
```
## 5. Exploration of data 
### Dimensions
```{r}
dim(students)
```
### Number of rows in data
```{r}
nrow(students)
```
### Number of columns
```{r}
ncol(students)
```
### Structure 
```{r}
str(students)
```
### Variable or column names 
```{r}
names(students)
```
### Attributes
```{r}
attributes(students)
```
### Top 10 rows
```{r}
head(students,n=10)
```
### Variable distribution before factorisation
```{r}
summary(students)
```
## Factorize continuous predictor variables
### Variable distribution after necessary factorisation
```{r}
students$Medu <- factor(students$Medu)
students$traveltime <- factor(students$traveltime)
summary(students)
```

### Pie chart for pass
```{r,echo=FALSE}
pie(with(students, table(Pass)))
```

### Pie chart for grade
```{r,echo=FALSE}
pie(with(students, table(Grade)))
```

### Bar graph for pass
```{r,echo=FALSE}
library(ggplot2)
ggplot(students, aes(x = Pass)) + geom_bar(fill="#FF9999")
```

### Bar graph for grade
```{r,echo=FALSE}
reordervect <- rep(0,nrow(students))
reordervect[with(students, Grade == "Fail")] = 1
reordervect[with(students, Grade == "Sufficient")] = 2
reordervect[with(students, Grade == "Satisfactory")] = 3
reordervect[with(students, Grade == "Good")] = 4
reordervect[with(students, Grade == "Excellent")] = 5
students$Grade = with(students, reorder(Grade, reordervect))
rm(reordervect)
ggplot(students, aes(x = Grade)) + geom_bar(fill="#FF9999") 
```

### Statistical data of G3
```{r}
summary(with(students,G3))
sprintf('variance is %f',var(with(students,G3)))
sprintf('standard deviation is %f',sd(with(students,G3)))
```

### Histogram for G3
```{r,echo=FALSE}
ggplot(students, aes(x=G3)) + geom_histogram(fill="#FF9999")
```

## Predicting G3 using G1

### Correlation between G1 and G3
```{r}
r <- cor(with(students,G1), with(students,G3))
sprintf("G3 shows a positive correllation with G1")
```
### Scatterplot of G1, G3
```{r,echo=FALSE}
ggplot(students, aes(x=G1,y=G3)) + geom_point() + geom_smooth(method="lm", se=FALSE)
```
### Fit linear regression using G1 as predictor to predict G3
```{r}
fit <- with(students,lm(G3 ~ G1))
fit
attributes(fit)
summary(fit)
```
### Plotting line of best fit
```{r,echo=FALSE}
plot(fit)
sprintf("Residual graph is random in nature suggesting linear regression is not a bad choice for this data")
```
## Predicting G3 using G2
### Correlation between G2 and G3
```{r}
r <- cor(with(students,G2), with(students,G3))
sprintf('Correlation between G2 and G3 is %f and the coefficient of determination is %f',r, r^2)
```
### Scatterplot of G2, G3
```{r,echo=FALSE}
ggplot(students, aes(x=G2,y=G3)) + geom_point() + geom_smooth(method="lm", se=FALSE)
```
### Fit linear regression using G1 as predictor to predict G3
```{r}
fit <- with(students,lm(G3 ~ G2))
fit
attributes(fit)
summary(fit)
```
### Plotting line of best fit
```{r,echo=FALSE}
plot(fit)
sprintf("Residual graph is random in nature suggesting linear regression is not a bad choice for this data")
```

## To predict  Pass and Fail using G1 + G2

### Using Decision Tree
```{r}
library(partykit)
formula <- Pass ~ G1 + G2
tree <- ctree(formula, data=students)
print(tree)
plot(tree,type = "simple")
```
```{r}
sprintf('Errors-on-predictions Matrix')
table(predict(tree, newdata=students), students$Pass,dnn=c('Predicted','Actual'))
df.confmatrix <- data.frame(table(predict(tree, newdata=students), students$Pass,dnn=c('Predicted','Actual')))
library(tidyr)
data_long <- gather(df.confmatrix, Type, Status, Predicted:Actual)
library(dplyr)
data_long <- data_long %>% group_by(Status,Type) %>% summarise(Frequency=sum(Freq))
ggplot(data_long, aes(x=Status,y=Frequency,fill=Type)) + geom_bar(stat='identity', position='dodge')
```

### Using Naive-Bayes Prediction

```{r}
library(e1071)
classifier<-naiveBayes(students[,1:4], students[,8])
table(predict(classifier, students[,1:4]), students[,8], dnn = c('Predicted','Actual'))
df.confmatrix <- data.frame(table(predict(classifier, students[,1:4]), students[,8], dnn = c('Predicted','Actual')))
data_long <- gather(df.confmatrix, Type, Status, Predicted:Actual)
data_long <- data_long %>% group_by(Status,Type) %>% summarise(Frequency=sum(Freq))
ggplot(data_long, aes(x=Status,y=Frequency,fill=Type)) + geom_bar(stat='identity', position='dodge')
```

## To predict Grades using G1+G3

### Using Decision Tree

```{r}
formula <- Grade ~ G1 + G2
tree <- ctree(formula, data=students)
print(tree)
plot(tree,type = "simple")
```
```{r}
sprintf('Errors-on-predictions Matrix')
table(predict(tree, newdata=students), students$Grade,dnn=c('Predicted','Actual'))
df.confmatrix <- data.frame(table(predict(tree, newdata=students), students$Grade,dnn=c('Predicted','Actual')))
data_long <- gather(df.confmatrix, Type, Status, Predicted:Actual)
data_long <- data_long %>% group_by(Status,Type) %>% summarise(Frequency=sum(Freq))
ggplot(data_long, aes(x=Status,y=Frequency,fill=Type)) + geom_bar(stat='identity', position='dodge')
```

### Using Naive-Bayes Prediction

```{r}
classifier<-naiveBayes(students[,1:4], students[,9])
sprintf('Errors-on-predictions Matrix')
table(predict(classifier, students[,1:4]), students[,9], dnn = c('Predicted','Actual'))
df.confmatrix <- data.frame(table(predict(classifier, students[,1:4]), students[,9], dnn = c('Predicted','Actual')))
data_long <- gather(df.confmatrix, Type, Status, Predicted:Actual)
data_long <- data_long %>% group_by(Status,Type) %>% summarise(Frequency=sum(Freq))
ggplot(data_long, aes(x=Status,y=Frequency,fill=Type)) + geom_bar(stat='identity', position='dodge') 
```

## Conclusion

Linear regression showed strong relationship between G3 and G2. G1 also showed positive relationship but not as strong as G2.
Decision tree prediction on the same dataset showed very less errors on predictions making G1 and G2 suitable for predicting Grades and Pass-ability of students
Naive Bayes method showed large errors on predictions. So, either the those four variables are not good predictors or Naive Bayes method is not a good predicting model for this dataset.
Based on all the analysis, G2 is the strongest predictor for G3, which in turn, for pass-ability and grades.